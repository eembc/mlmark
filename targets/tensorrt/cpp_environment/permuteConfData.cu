/*
 * Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <vector>
#include <cstring>
#include <stdint.h>
#include <iostream>

#include "ssdOpt.h"
#include "ssdOptMacros.h"
//#include "ssd_internal.h"

namespace nvinfer1
{
namespace plugin
{

////////////////////////////////////////////////////////////////////////////////////////////////////

static inline int clz(int x) {
    for( int i = 31; i >= 0; --i ) {
        if( (1 << i) & x ) { 
            return 31 - i;
        }
    }
    return 32;
}
////////////////////////////////////////////////////////////////////////////////////////////////////

static int find_log_2(int x, bool round_up = false) {
    int a = 31 - clz(x);
    if( round_up ) {
        a += (x & (x-1)) ? 1 : 0;
    }
    return a;
}
////////////////////////////////////////////////////////////////////////////////////////////////////

static void find_divisor(uint32_t &mul, uint32_t &shr, int x) {
    assert(x != 0);
    if( x == 1 ) {
        // If dividing by 1, reduced math doesn't work because mul_coeff would need to be 2^32,
        // which doesn't fit into unsigned int.  the div() routine handles this special case
        // separately.
        mul = 0;
        shr = 0;
    } else {
        // To express the division N/D in terms of a multiplication, what we first
        // imagine is simply N*(1/D).  However, 1/D will always evaluate to 0 (for D>1),
        // so we need another way.  There's nothing that says we have to use exactly
        // the fraction 1/D; instead it could be any X/Y that reduces to 1/D (i.e.,
        // Y=X*D), or at least to "close enough" to it.  If we pick Y that is a power
        // of two, then the N*(X/Y) can be N*X followed by a right-shift by some amount.
        // The power of two we should pick should be at least 2^32, because in the
        // div() routine we'll use umulhi(), which returns only the upper 32 bits --
        // this being equivalent to a right-shift by 32.  But we might want a higher
        // power of two for better accuracy depending on the magnitude of the denominator.
        // Once we've picked Y, then X [our mul_coeff value] is simply Y/D, rounding up,
        // and we save shift_coeff as whatever further shift we have to do beyond
        // what the umulhi() implies.
        uint32_t p = 31 + find_log_2(x, true);
        uint32_t m = ((1ull << p) + (uint32_t) x - 1) / (uint32_t) x;

        mul = m;
        shr = p - 32;
    }
}

////////////////////////////////////////////////////////////////////////////////////////////////////

static inline __device__
void fast_divmod(int &div, int &mod, int x, int y, uint32_t mul, uint32_t shr) {
    #if 0
    if( y == 1 ) {
        div = x;
        mod = 0;
    } else {
        div = __umulhi((uint32_t) x, mul) >> shr;
        mod = x - div*y;
    }
    #elif 1
    div = x;
    mod = 0;
    if (y != 1) {
        div = __umulhi((uint32_t) x, mul) >> shr;
        mod = x - div*y;
    }
    #else
    div = (y != 1) ? __umulhi((uint32_t) x, mul) >> shr : x;
    mod = (y != 1) ? (x - div*y) : 0;

    #endif
}

template <typename Dtype, int NUM_LAYERS>
struct PermuteConfData {
    const Dtype * conf_data[NUM_LAYERS];
    int feature_size[NUM_LAYERS];
    int num_anchors[NUM_LAYERS];
    int end_layer_prior[NUM_LAYERS];
    bool packed32_nchw;
};

/* This function maps the input index to the corresponding conf_data offset.
The input "conf_data" is composed of "num_layers" conf tensors from the CONV
layers in SSD. These tensors are in NCHW layout. 
The input index is broken down to 4 components: i, c, d, n
i - box coordinate (max 4)
c - class (max num_classes)
d - prior (max num_priors)
n - batch size

The transformed conf_data is generated by:
conf_data[id_layer](CHW)->permute(1,2,0)(HWC)->reshape(H*W*C/num_classes/num_dims, num_classes, num_dims)
->concat(axis=1, num_layers)(num_priors, num_classes, num_dims)[->flatten(num_priors * num_classes * num_dims, 1, 1)]->permute(num_classes, num_priors, num_dims)
Correspondingly, the mapping process will first locate id_layer according to prior and then transform
the index based on (num_classes, num_priors, num_dims) backed to CHW.
C = num_anchors_layer * num_classes * num_dims
HW = num_priors_layer / num_anchors_layer
*/


template <typename Dtype, unsigned nthds_per_cta, int NUM_LAYERS>
//__launch_bounds__(nthds_per_cta)
__launch_bounds__(128)
__global__ void permuteConfData_kernel(
        const int nthreads,
        const int num_classes, int num_classes_mul, int num_classes_shr,
        const int num_priors, int num_priors_mul, int num_priors_shr,
        const int num_dim, int num_dim_mul, int num_dim_shr,
        int fast_divmod3_mul, int fast_divmod3_shr,
        int fast_divmod6_mul, int fast_divmod6_shr,
        int fast_divmod4_mul, int fast_divmod4_shr,
        bool confSigmoid,
        Dtype* new_data,
        int *active_counts_per_class,
        const PermuteConfData<Dtype, NUM_LAYERS> permute_conf_data)
{
    int feature_size[NUM_LAYERS];
    int all_num_anchors[NUM_LAYERS];
    const Dtype *conf_data[NUM_LAYERS];

    #pragma unroll
    for (int layer = 0;layer < NUM_LAYERS;++layer) {
        feature_size[layer] = permute_conf_data.feature_size[layer];
        all_num_anchors[layer] = permute_conf_data.num_anchors[layer];
        conf_data[layer] = permute_conf_data.conf_data[layer];
    }
    const bool packed32_nchw = permute_conf_data.packed32_nchw;

    int index = blockIdx.x * nthds_per_cta + threadIdx.x;
    {
        int i, i_div, d, d_div, c, n;

        fast_divmod(i_div, i, index, num_dim, num_dim_mul, num_dim_shr);
        fast_divmod(d_div, d, i_div, num_priors, num_priors_mul, num_priors_shr);
        fast_divmod(n, c, d_div, num_classes, num_classes_mul, num_classes_shr);

        if (n == 0) {
            active_counts_per_class[n] = 0;
        }

        //find layer_id
        int start_layer_prior = 0, end_layer_prior = 0;
        int prior_in_layer = 0;
        const Dtype *conf_data_layer;

        int num_hw;
        int layer;
        int num_anchors;
        #pragma unroll
        for(layer = 0; layer < NUM_LAYERS; layer++) {
            end_layer_prior = permute_conf_data.end_layer_prior[layer];

            if(d < end_layer_prior) {
                conf_data_layer = conf_data[layer];
                num_hw = feature_size[layer];

                num_anchors = all_num_anchors[layer];

                prior_in_layer = d - start_layer_prior;

                d = INT_MAX;
            }
            start_layer_prior = end_layer_prior;
        }

        int mappedIndex;
        int hw = prior_in_layer % num_hw;
        int anchor = prior_in_layer / num_hw;

        int num_ch = num_dim * num_classes * num_anchors; 
        int ch = (i*num_classes + c)*num_anchors + anchor;

        if(packed32_nchw) {
            int packed_num_ch = (num_ch+31)/32;
            
            int packed_ch = ch >> 5; // ch/32;
            int packed_ch_offset = ch & 31; // ch%32;

            mappedIndex = ((n * packed_num_ch + packed_ch)*num_hw + hw)*32 + packed_ch_offset;
        }
        else {
            mappedIndex = (n * num_ch + ch)*num_hw + hw;
        }
    
        float result = conf_data_layer[mappedIndex];

        if (confSigmoid)
            result = __expf(result) / (1 + __expf(result));

        new_data[index] = result;
    }
}

template <typename Dtype>
ssdStatus_t permuteConfData_gpu(
    cudaStream_t stream,
    const int nthreads,
    const int num_classes,
    const int num_priors,
    const int num_dim,
    bool confSigmoid,
    const void* const* conf_data,
    void* new_data,
    void* active_count_per_class,
    const int num_layers,
    const int* feature_size,
    const int* num_anchors,
    const bool packed32_nchw)
{
    const int BS = 128;
    const int GS = (nthreads + BS - 1) / BS;
    if(num_layers == 6) { // handle a special case
        PermuteConfData<Dtype, 6> permute_conf_data;

        // precompute pow2(feature_size) and end_prior_layer for each loop iteration.
        int start_layer_prior = 0;
        for (int i = 0;i < num_layers;++i) {
            permute_conf_data.feature_size[i] = feature_size[i] * feature_size[i];
            permute_conf_data.num_anchors[i] = num_anchors[i];

            int layer_prior_size = num_anchors[i] * permute_conf_data.feature_size[i];
            int end_layer_prior = start_layer_prior + layer_prior_size;

            permute_conf_data.end_layer_prior[i] = end_layer_prior;
            start_layer_prior = end_layer_prior;
        }

        permute_conf_data.packed32_nchw = packed32_nchw;

        // determine constants for efficient integer division
        uint32_t num_classes_mul, num_classes_shr;
        uint32_t num_priors_mul, num_priors_shr;
        uint32_t num_dim_mul, num_dim_shr;
        find_divisor(num_classes_mul, num_classes_shr, num_classes);
        find_divisor(num_priors_mul, num_priors_shr, num_priors);
        find_divisor(num_dim_mul, num_dim_shr, num_dim);

        uint32_t fast_divmod_3_mul, fast_divmod_3_shr;
        uint32_t fast_divmod_6_mul, fast_divmod_6_shr;
        uint32_t fast_divmod_4_mul, fast_divmod_4_shr;
        find_divisor(fast_divmod_3_mul, fast_divmod_3_shr, 3);
        find_divisor(fast_divmod_6_mul, fast_divmod_6_shr, 6);
        find_divisor(fast_divmod_4_mul, fast_divmod_4_shr, 4);

        std::memcpy(permute_conf_data.conf_data, conf_data, 6 * sizeof(void*));
        permuteConfData_kernel<Dtype, BS, 6><<<GS, BS, 0, stream>>>(nthreads,
                                                                    num_classes, num_classes_mul, num_classes_shr,
                                                                    num_priors, num_priors_mul, num_priors_shr,
                                                                    num_dim, num_dim_mul, num_dim_shr,
                                                                    fast_divmod_3_mul, fast_divmod_3_shr,
                                                                    fast_divmod_6_mul, fast_divmod_6_shr,
                                                                    fast_divmod_4_mul, fast_divmod_4_shr,
                                                                    confSigmoid,
                                                                    (Dtype*) new_data, (int*) active_count_per_class, permute_conf_data);
    }
    else{
        std::cerr<< "Only support numLayers == 6" << std::endl;
        return STATUS_FAILURE;
    }
    CSC(cudaGetLastError(), STATUS_FAILURE);
    return STATUS_SUCCESS;
}

// permuteConfData LAUNCH CONFIG {{{
typedef ssdStatus_t (*pdFunc)(cudaStream_t,
                              const int,
                              const int,
                              const int,
                              const int,
                              bool,
                              const void* const*,
                              void*,
                              void*,
                              const int,
                              const int*,
                              const int*,
                              const bool);

struct pdLaunchConfig
{
    DType_t t_data;
    pdFunc function;

    pdLaunchConfig(DType_t t_data)
        : t_data(t_data)
    {
    }
    pdLaunchConfig(DType_t t_data, pdFunc function)
        : t_data(t_data)
        , function(function)
    {
    }
    bool operator==(const pdLaunchConfig& other)
    {
        return t_data == other.t_data;
    }
};

static std::vector<pdLaunchConfig> pdFuncVec;

bool permuteConfDataInit()
{
    pdFuncVec.push_back(pdLaunchConfig(DataType::kFLOAT,
                                       permuteConfData_gpu<float>));
    return true;
}

static bool initialized = permuteConfDataInit();

//}}}

ssdStatus_t permuteConfData(cudaStream_t stream,
                        const int nthreads,
                        const int num_classes,
                        const int num_priors,
                        const int num_dim,
                        const DType_t DT_DATA,
                        bool confSigmoid,
                        const void* const* conf_data,
                        void* new_data,
                        void* active_classes_per_batch,
                        const int num_layers,
                        const int * feature_size,
                        const int * num_anchors,
                        const bool packed32_nchw)
{
    pdLaunchConfig lc = pdLaunchConfig(DT_DATA);
    for (unsigned i = 0; i < pdFuncVec.size(); ++i)
    {
        if (lc == pdFuncVec[i])
        {
            DEBUG_PRINTF("permuteConfData kernel %d\n", i);
            return pdFuncVec[i].function(stream,
                                         nthreads,
                                         num_classes,
                                         num_priors,
                                         num_dim,
                                         confSigmoid,
                                         conf_data,
                                         new_data,
                                         active_classes_per_batch,
                                         num_layers,
                                         feature_size,
                                         num_anchors,
                                         packed32_nchw);
        }
    }
    return STATUS_BAD_PARAM;
}

} // namespace plugin
} // namespace nvinfer1
